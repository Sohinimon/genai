Abstract:

The article discusses "EvalPlus," a new framework for evaluating the effectiveness of Large Language Models (LLMs) like GPT-4 and ChatGPT in code synthesis. 
Challenges :
Traditional benchmarks for measuring LLM performance in code generation often have limitations in test-cases, both in quantity and quality. 

This limitation raises doubts about the functional correctness of the code produced by these models.
EvalPlus addresses this by enhancing existing evaluation datasets with a significantly larger number of test cases generated through both LLM- and mutation-based strategies. The authors demonstrate this by expanding the HUMANEVAL benchmark into HUMANEVAL+, increasing its test cases by 80 times.
Their extensive evaluation of 26 popular LLMs revealed that HUMANEVAL+ could detect much more incorrect code generated by LLMs than the original benchmark, with a reduction in pass rates of up to 19.3-28.9%. Interestingly, they found that insufficient testing could lead to incorrect ranking of LLMs' performance. For example, models like WizardCoder-CodeLlama and Phind-CodeLlama, which previously underperformed compared to ChatGPT on HUMANEVAL, showed better performance on HUMANEVAL+.
The study highlights that previous benchmarks may not accurately reflect the true capabilities of LLMs in code synthesis. It also suggests a new direction for improving programming benchmarks through automated testing. The authors have made their tools, enhanced datasets, and LLM-generated code available open-source for future research in LLM-for-code.

Program synthesis is the task of automatically ﬁnding a program in the underlying programming language that satisﬁes the user intent expressed in the form of some speciﬁcation. Since the inception of AI in the 1950s, this problem has been considered the holy grail of Computer Science. 


Objective: 
1.	Evaluate the evaluation dataset. 
2.	Build EvalPlus – an evaluation framework to improve existing code benchmarks in order to precisely evaluate the functional correctness of LLM-generated code

•	EvalPlus is an automatic test input generation engine which augments existing code benchmarks by generating interesting test inputs 
•	Check its functional correctness by cross-checking the ground-truth implementation. 
•	EvalPlus adopts both LLM- and mutation-based [57, 74, 47] methods to automatically generate and diversify additional test inputs.
•	EvalPlus first uses ChatGPT [48] to generate a set of high-quality seed inputs that aim to test difficult corner cases and functionalities of the program within the valid input structure. 
•	Using these high-quality seed inputs, EvalPlus then performs type-aware mutation to efficiently generate a large number of additional test inputs.

Process
1.	Starting with ChatGPT: EvalPlus first uses ChatGPT to create a set of initial test inputs, called seed inputs. To do this, it gives ChatGPT (i) the correct solution to a problem, (ii) some example test inputs, and (iii) instructions to come up with new and interesting inputs. The idea is to use ChatGPT's understanding to make test inputs that are realistic and challenging.
2.	Creating More Test Inputs with Mutation: After getting these seed inputs from ChatGPT, EvalPlus uses a method called type-aware input mutation. This process takes the seed inputs and slightly alters them, considering the type of data (like numbers, text, etc.). This way, it quickly creates many new test inputs that are similar but not identical to the original seeds.
3.	Filtering Inputs: EvalPlus filters out any inputs that don't fit the expected format or rules of the problem. This is important because wrong inputs can give misleading results.
4.	Why Use ChatGPT and Mutation Together: ChatGPT is great for creating inputs that are not only correct but also fit complex requirements, like a specific format or rule. However, it's not ideal for making a huge number of tests quickly. That's where mutation comes in – it takes the good inputs from ChatGPT and makes many variations of them efficiently.
5.	Final Goal: The ultimate aim of this process is to have a large and varied set of test inputs to check if the code generated by LLMs (like GPT-4) is really working correctly in all sorts of different scenarios.


Test-Suite Reduction in EvalPlus
•	Reducing Test Numbers for Efficiency: EvalPlus creates a lot of tests to find errors in code, but running all these tests can take a lot of time and resources. To make things more efficient, it looks for ways to use fewer tests while still being effective.

•	Choosing the Best Subset of Tests: The idea is to pick a smaller group of tests from the original large set. This smaller group should still be able to find the same problems that the larger set could.

•	Using Set Covering Approach: This process is similar to solving a puzzle where you try to cover all needed areas with as few pieces as possible. EvalPlus aims to cover all testing needs with the least number of tests.

Evaluation:
1.Comprehensive LLM Evaluation: EvalPlus conducted an extensive evaluation of 26 popular LLMs, including GPT-4 and ChatGPT, using both random sampling and greedy-search decoding across different temperature settings. This was done to accurately assess the functional correctness of code synthesized by these models.

2.Enhancing HUMANEVAL Dataset: The study focused on the HUMANEVAL dataset, widely used for code generation. EvalPlus expanded this dataset into HUMANEVAL+ by adding 80 times more test cases and correcting over 10% of incorrect ground-truth solutions. Additionally, a smaller yet equally effective version, HUMANEVAL+-MINI, was created for faster evaluation.

3.Detailed Methodology for Test Input Generation: For each programming task, EvalPlus generated around 30 seed inputs using ChatGPT and then applied type-aware mutation to produce 1000 additional inputs. This approach ensures a thorough and rigorous evaluation of the LLM-generated code.
